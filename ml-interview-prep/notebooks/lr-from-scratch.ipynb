{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b08eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4951b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7767c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8a9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X[:,:2]\n",
    "targets = (Y !=0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa13ad60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# test sigmoid\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35210a",
   "metadata": {},
   "source": [
    "### Binary LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3873e02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 1 is 1.2045817136496513\n",
      "loss at epoch 2 is 1.1474830575339074\n",
      "loss at epoch 3 is 1.0964386185923212\n",
      "loss at epoch 4 is 1.047806595608238\n",
      "loss at epoch 5 is 1.0014626092432484\n",
      "loss at epoch 6 is 0.9572989968586594\n",
      "loss at epoch 7 is 0.9152167720641914\n",
      "loss at epoch 8 is 0.8751245214880472\n",
      "loss at epoch 9 is 0.8369373747144894\n",
      "loss at epoch 10 is 0.800576045811472\n",
      "loss at epoch 11 is 0.7659659553949013\n",
      "loss at epoch 12 is 0.7330364399600852\n",
      "loss at epoch 13 is 0.7017200537810475\n",
      "loss at epoch 14 is 0.6719519675479712\n",
      "loss at epoch 15 is 0.6436694665134974\n",
      "loss at epoch 16 is 0.6168115488950967\n",
      "loss at epoch 17 is 0.5913186225933253\n",
      "loss at epoch 18 is 0.56713229517083\n",
      "loss at epoch 19 is 0.5441952488899016\n",
      "loss at epoch 20 is 0.5224511898512141\n",
      "loss at epoch 21 is 0.5018448582597123\n",
      "loss at epoch 22 is 0.4823220857717837\n",
      "loss at epoch 23 is 0.46382988579887374\n",
      "loss at epoch 24 is 0.446316563463216\n",
      "loss at epoch 25 is 0.42973183342713595\n",
      "loss at epoch 26 is 0.41402693580332134\n",
      "loss at epoch 27 is 0.3991547425493325\n",
      "loss at epoch 28 is 0.38506984893702473\n",
      "loss at epoch 29 is 0.3717286466997775\n",
      "loss at epoch 30 is 0.3590893771902924\n",
      "loss at epoch 31 is 0.34711216427747343\n",
      "loss at epoch 32 is 0.33575902776479205\n",
      "loss at epoch 33 is 0.3249938788493731\n",
      "loss at epoch 34 is 0.3147824996031912\n",
      "loss at epoch 35 is 0.30509250869670995\n",
      "loss at epoch 36 is 0.2958933156524632\n",
      "loss at epoch 37 is 0.2871560658588045\n",
      "loss at epoch 38 is 0.2788535784325808\n",
      "loss at epoch 39 is 0.27096027882659734\n",
      "loss at epoch 40 is 0.2634521278583182\n",
      "loss at epoch 41 is 0.25630654860887714\n",
      "loss at epoch 42 is 0.24950235241882404\n",
      "loss at epoch 43 is 0.24301966499757674\n",
      "loss at epoch 44 is 0.2368398534722913\n",
      "loss at epoch 45 is 0.23094545503112124\n",
      "loss at epoch 46 is 0.2253201076666285\n",
      "loss at epoch 47 is 0.21994848339654122\n",
      "loss at epoch 48 is 0.2148162242303519\n",
      "loss at epoch 49 is 0.20990988105929592\n",
      "loss at epoch 50 is 0.20521685557250813\n",
      "loss at epoch 51 is 0.20072534524148763\n",
      "loss at epoch 52 is 0.19642429136647654\n",
      "loss at epoch 53 is 0.19230333014053258\n",
      "loss at epoch 54 is 0.18835274665779025\n",
      "loss at epoch 55 is 0.1845634317707871\n",
      "loss at epoch 56 is 0.18092684168603962\n",
      "loss at epoch 57 is 0.1774349601764705\n",
      "loss at epoch 58 is 0.17408026328272488\n",
      "loss at epoch 59 is 0.17085568637209622\n",
      "loss at epoch 60 is 0.16775459342300086\n",
      "loss at epoch 61 is 0.16477074840421102\n",
      "loss at epoch 62 is 0.16189828862076322\n",
      "loss at epoch 63 is 0.15913169990232948\n",
      "loss at epoch 64 is 0.1564657935145748\n",
      "loss at epoch 65 is 0.15389568467915885\n",
      "loss at epoch 66 is 0.15141677259373468\n",
      "loss at epoch 67 is 0.14902472184897556\n",
      "loss at epoch 68 is 0.14671544514555734\n",
      "loss at epoch 69 is 0.14448508721976955\n",
      "loss at epoch 70 is 0.14233000989212566\n",
      "loss at epoch 71 is 0.1402467781588674\n",
      "loss at epoch 72 is 0.1382321472515415\n",
      "loss at epoch 73 is 0.13628305059490914\n",
      "loss at epoch 74 is 0.13439658859826334\n",
      "loss at epoch 75 is 0.13257001821980324\n",
      "loss at epoch 76 is 0.13080074324800545\n",
      "loss at epoch 77 is 0.1290863052479695\n",
      "loss at epoch 78 is 0.12742437512448967\n",
      "loss at epoch 79 is 0.12581274525717304\n",
      "loss at epoch 80 is 0.12424932216617116\n",
      "loss at epoch 81 is 0.12273211967019661\n",
      "loss at epoch 82 is 0.1212592525013325\n",
      "loss at epoch 83 is 0.11982893034378796\n",
      "loss at epoch 84 is 0.11843945226621026\n",
      "loss at epoch 85 is 0.11708920151944895\n",
      "loss at epoch 86 is 0.11577664067372823\n",
      "loss at epoch 87 is 0.11450030707119163\n",
      "loss at epoch 88 is 0.11325880857154123\n",
      "loss at epoch 89 is 0.11205081957015181\n",
      "loss at epoch 90 is 0.11087507726960813\n",
      "loss at epoch 91 is 0.10973037818701124\n",
      "loss at epoch 92 is 0.10861557488070503\n",
      "loss at epoch 93 is 0.10752957288129814\n",
      "loss at epoch 94 is 0.1064713278129679\n",
      "loss at epoch 95 is 0.1054398426920555\n",
      "loss at epoch 96 is 0.10443416539092945\n",
      "loss at epoch 97 is 0.1034533862559519\n",
      "loss at epoch 98 is 0.10249663586921487\n",
      "loss at epoch 99 is 0.10156308294444885\n",
      "loss at epoch 100 is 0.10065193234819961\n",
      "loss at epoch 101 is 0.09976242323801969\n",
      "loss at epoch 102 is 0.09889382730998225\n",
      "loss at epoch 103 is 0.09804544714842083\n",
      "loss at epoch 104 is 0.0972166146712457\n",
      "loss at epoch 105 is 0.09640668966469015\n",
      "loss at epoch 106 is 0.09561505840177223\n",
      "loss at epoch 107 is 0.09484113233913441\n",
      "loss at epoch 108 is 0.09408434688730724\n",
      "loss at epoch 109 is 0.0933441602497933\n",
      "loss at epoch 110 is 0.0926200523266593\n",
      "loss at epoch 111 is 0.09191152367863749\n",
      "loss at epoch 112 is 0.09121809454802653\n",
      "loss at epoch 113 is 0.09053930393286588\n",
      "loss at epoch 114 is 0.08987470871118507\n",
      "loss at epoch 115 is 0.08922388281226448\n",
      "loss at epoch 116 is 0.08858641643210201\n",
      "loss at epoch 117 is 0.08796191529042668\n",
      "loss at epoch 118 is 0.08734999992680449\n",
      "loss at epoch 119 is 0.08675030503351888\n",
      "loss at epoch 120 is 0.08616247882308117\n",
      "loss at epoch 121 is 0.08558618242833695\n",
      "loss at epoch 122 is 0.08502108933329218\n",
      "loss at epoch 123 is 0.08446688483289404\n",
      "loss at epoch 124 is 0.08392326552009098\n",
      "loss at epoch 125 is 0.08338993879864622\n",
      "loss at epoch 126 is 0.08286662242022912\n",
      "loss at epoch 127 is 0.08235304404443688\n",
      "loss at epoch 128 is 0.08184894082045835\n",
      "loss at epoch 129 is 0.08135405898918212\n",
      "loss at epoch 130 is 0.08086815350462043\n",
      "loss at epoch 131 is 0.08039098767360041\n",
      "loss at epoch 132 is 0.07992233281271609\n",
      "loss at epoch 133 is 0.0794619679216144\n",
      "loss at epoch 134 is 0.0790096793717361\n",
      "loss at epoch 135 is 0.07856526060968315\n",
      "loss at epoch 136 is 0.07812851187444597\n",
      "loss at epoch 137 is 0.07769923992773886\n",
      "loss at epoch 138 is 0.07727725779678188\n",
      "loss at epoch 139 is 0.07686238452885394\n",
      "loss at epoch 140 is 0.07645444495702726\n",
      "loss at epoch 141 is 0.07605326947649949\n",
      "loss at epoch 142 is 0.07565869383097486\n",
      "loss at epoch 143 is 0.07527055890859571\n",
      "loss at epoch 144 is 0.07488871054692763\n",
      "loss at epoch 145 is 0.07451299934655449\n",
      "loss at epoch 146 is 0.07414328049284141\n",
      "loss at epoch 147 is 0.07377941358547223\n",
      "loss at epoch 148 is 0.07342126247536429\n",
      "loss at epoch 149 is 0.07306869510860957\n",
      "loss at epoch 150 is 0.07272158337708959\n",
      "loss at epoch 151 is 0.07237980297544512\n",
      "loss at epoch 152 is 0.07204323326409488\n",
      "loss at epoch 153 is 0.07171175713800629\n",
      "loss at epoch 154 is 0.07138526090095511\n",
      "loss at epoch 155 is 0.07106363414499689\n",
      "loss at epoch 156 is 0.07074676963492371\n",
      "loss at epoch 157 is 0.07043456319745413\n",
      "loss at epoch 158 is 0.0701269136149501\n",
      "loss at epoch 159 is 0.06982372252344096\n",
      "loss at epoch 160 is 0.06952489431475924\n",
      "loss at epoch 161 is 0.069230336042602\n",
      "loss at epoch 162 is 0.06893995733233528\n",
      "loss at epoch 163 is 0.06865367029437446\n",
      "loss at epoch 164 is 0.06837138944097183\n",
      "loss at epoch 165 is 0.0680930316062727\n",
      "loss at epoch 166 is 0.06781851586948046\n",
      "loss at epoch 167 is 0.0675477634809996\n",
      "loss at epoch 168 is 0.06728069779142357\n",
      "loss at epoch 169 is 0.06701724418324122\n",
      "loss at epoch 170 is 0.06675733000514901\n",
      "loss at epoch 171 is 0.06650088450884509\n",
      "loss at epoch 172 is 0.06624783878821075\n",
      "loss at epoch 173 is 0.0659981257207637\n",
      "loss at epoch 174 is 0.06575167991129946\n",
      "loss at epoch 175 is 0.06550843763761631\n",
      "loss at epoch 176 is 0.06526833679824201\n",
      "loss at epoch 177 is 0.06503131686207653\n",
      "loss at epoch 178 is 0.06479731881987062\n",
      "loss at epoch 179 is 0.06456628513746358\n",
      "loss at epoch 180 is 0.06433815971070916\n",
      "loss at epoch 181 is 0.06411288782201387\n",
      "loss at epoch 182 is 0.0638904160984335\n",
      "loss at epoch 183 is 0.06367069247124996\n",
      "loss at epoch 184 is 0.06345366613698034\n",
      "loss at epoch 185 is 0.06323928751975251\n",
      "loss at epoch 186 is 0.06302750823499839\n",
      "loss at epoch 187 is 0.06281828105440554\n",
      "loss at epoch 188 is 0.06261155987208517\n",
      "loss at epoch 189 is 0.06240729967189993\n",
      "loss at epoch 190 is 0.062205456495911866\n",
      "loss at epoch 191 is 0.0620059874139065\n",
      "loss at epoch 192 is 0.06180885049394665\n",
      "loss at epoch 193 is 0.061614004773919435\n",
      "loss at epoch 194 is 0.061421410234038236\n",
      "loss at epoch 195 is 0.06123102777026303\n",
      "loss at epoch 196 is 0.06104281916860109\n",
      "loss at epoch 197 is 0.06085674708025773\n",
      "loss at epoch 198 is 0.06067277499760591\n",
      "loss at epoch 199 is 0.06049086723093776\n",
      "Accuracy of the model is 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "w = np.random.rand(2)\n",
    "b = 1\n",
    "\n",
    "for i in range(1,200):\n",
    "    for x,y in zip(features,targets):\n",
    "        \n",
    "        # get prediction of that point\n",
    "        output = sigmoid(np.dot(x,w)+b)\n",
    "        \n",
    "        # calculate error\n",
    "        error = (y - output)\n",
    "\n",
    "        # update weights and bias\n",
    "        w += error * alpha * x\n",
    "        b += error * alpha\n",
    "    \n",
    "    # preds for that epoch\n",
    "    y_hat = sigmoid(np.dot(features,w)+b)\n",
    "    \n",
    "    # loss at epoch - Negative log likelihood\n",
    "    loss = - ( (targets * np.log(y_hat)) + ((1-targets) * np.log(1-y_hat)))\n",
    "    print('loss at epoch {0} is {1}'.format(i, np.mean(loss)))\n",
    "\n",
    "# final preds after training\n",
    "y_hat_final = sigmoid(np.dot(features,w)+b)\n",
    "final_preds = y_hat_final > 0.5\n",
    "\n",
    "# accuracy\n",
    "print('Accuracy of the model is {0}'.format(np.mean(final_preds == targets)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89d4ca",
   "metadata": {},
   "source": [
    "### Multi class LR - sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "0b61b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "de05c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)\n",
    "lr_model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "0e6bd938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "preds = lr_model.predict(X_test)\n",
    "print('Accuracy of the model is {0}'.format(np.mean(preds == y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779769d",
   "metadata": {},
   "source": [
    "### Multi-class LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "e3f31a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/ np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "f24e20a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y_enc = OneHotEncoder().fit_transform(Y.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "5d3b3e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "w = np.random.rand(4,3)\n",
    "alpha = 0.001\n",
    "\n",
    "for i in range(1,400):\n",
    "    for x,y in zip(X,y_enc):        \n",
    "        # get prediction of that point\n",
    "        output = softmax(np.matmul(x.reshape(1,4),w))\n",
    "        \n",
    "        # calculate error\n",
    "        error = (y - output)\n",
    "        error_term = np.matmul(x.reshape(4,1),error)\n",
    "\n",
    "        # update weights\n",
    "        w += error_term * alpha\n",
    "\n",
    "    # preds for that epoch\n",
    "    y_hat = softmax(np.matmul(X,w))\n",
    "    \n",
    "    # loss at epoch - milticlass cross entropy\n",
    "    loss = - (np.sum(y_enc * np.log(y_hat)))/X.shape[0]   \n",
    "    # print('loss at epoch {0} is {1}'.format(i, loss))\n",
    "\n",
    "    \n",
    "y_hat_final = softmax(np.matmul(X,w))\n",
    "print(np.mean(Y == np.argmax(y_hat_final,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e7977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b6a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
